{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssdeep hash: scalable pairwise comparisons in pure Python\n",
    "\n",
    "\n",
    "This notebook contains the following:\n",
    "* A class called ```DocHash``` which pairs an input text string/document with an index, and an ssdeep hash value;\n",
    "* A random document generator for testing the performance of ```DocHash``` comparison algorithms, below.\n",
    "\n",
    "Three algorithms are implemented to test the scalability of pairwise ```DocHash``` comparisons:\n",
    "* A naive pairwise ```DocHash``` instance comparator, comparing all n(n-1)/2 pairs in a list of n instances, to construct a baseline of both correct matches, and a time to compute all pairs.\n",
    "* A slightly improved naive pairwise test, which tests the optimisation that ssdeep hash pairs should only be compared if the chunksizes are equal, double, or half (see https://www.intezer.com/blog/malware-analysis/intezer-community-tip-ssdeep-comparisons-with-elasticsearch/)\n",
    "* An efficient ```DocHash``` ssdeep hash matching algorithm which constructs a set of all matching/similar pairs (documentation below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssdeep\n",
    "\n",
    "class DocHash:\n",
    "    \"\"\"An indexed document and ssdeep hash pair\"\"\"\n",
    "    def __init__(self, index, doc):\n",
    "        self.index = index\n",
    "        self.doc = doc\n",
    "        self.hash = ssdeep.hash(doc)\n",
    "        self.chunksize = int(self.hash.split(':', 1)[0])\n",
    "        self.chunk_ngrams = set(self._ngrams(self.hash.split(':', 2)[1], ngram_len=7))\n",
    "        self.double_chunk_ngrams = set(self._ngrams(self.hash.split(':', 2)[2], ngram_len=7))\n",
    "        \n",
    "    def get_index(self):\n",
    "        return self.index\n",
    "\n",
    "    def get_doc(self):\n",
    "        return self.doc\n",
    "\n",
    "    def get_hash(self):\n",
    "        return self.hash\n",
    "\n",
    "    def get_chunksize(self):\n",
    "        return self.chunksize\n",
    "    \n",
    "    def _ngrams(self, s, ngram_len=7):\n",
    "        if len(s) < ngram_len:\n",
    "            return [s]\n",
    "        else:\n",
    "            # generate all ngrams of length ngraph_len from s\n",
    "            return [s[i:i+ngram_len] for i in range(0, len(s)) if i + ngram_len <= len(s)]\n",
    "            \n",
    "    def get_chunk_ngrams(self):\n",
    "        return self.chunk_ngrams\n",
    "\n",
    "    def get_double_chunk_ngrams(self):\n",
    "        return self.double_chunk_ngrams\n",
    "        \n",
    "    def compare(self, that):\n",
    "        return ssdeep.compare(self.hash, that.get_hash())\n",
    "    \n",
    "    def comparable(self, that):\n",
    "        c0 = self.chunksize\n",
    "        c1 = that.chunksize\n",
    "        if(c0 > c1):\n",
    "            c0 = that.chunksize\n",
    "            c1 = self.chunksize\n",
    "        # Rule: Only compare hashes that have chunksize equal, double or half of the chunksize of the other\n",
    "        return c0 == c1 or c0 * 2 == c1\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str((self.doc, self.hash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunksizes {(chunksize: count)}: {48: 5167, 24: 2044, 3: 388, 12: 1211, 96: 723, 6: 467}\n"
     ]
    }
   ],
   "source": [
    "# Computes a list of DocHash instances based on randomly generated text.\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "# seed the random generator for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# test parameters\n",
    "NUM_DOCS = 10000\n",
    "SEED_SENTENCE = \"This is a seed sentence which will be randomly permuted for input to the ssdhash comparison test.\"\n",
    "MIN_SENTENCE_LENGTH = 1\n",
    "MIN_SENTENCES_PER_DOC = 10\n",
    "MAX_SENTENCES_PER_DOC = 100\n",
    "MAX_PERMUTATIONS = 5\n",
    "\n",
    "# a random permutaion method which randomly permutes 'permutations' characters of an the input string, s  \n",
    "def random_permute(s, permutations):\n",
    "    if permutations <= 0:\n",
    "        # no more permutations to be computed, return s\n",
    "        return s\n",
    "    else:\n",
    "        # index to permute\n",
    "        index = random.randrange(len(s))\n",
    "        # permute the string at the randomly chosen index\n",
    "        s = s[:index] + ''.join(random.choices(string.ascii_lowercase, k=1)) + s[index + 1:]\n",
    "        # permute some more\n",
    "        return random_permute(s, permutations - 1)\n",
    "\n",
    "# the random document generator    \n",
    "docs = []\n",
    "for i in range(0, NUM_DOCS):\n",
    "    doc = ''.join([random_permute(SEED_SENTENCE[:random.randint(MIN_SENTENCE_LENGTH, len(SEED_STRING))], random.randint(1, MAX_PERMUTATIONS)) for i in range(MIN_SENTENCES_PER_DOC, random.randint(MIN_SENTENCES_PER_DOC + 1, MAX_SENTENCES_PER_DOC))])\n",
    "    docs.append(DocHash(i, doc))\n",
    "    \n",
    "# print some stats for the randomly generated list of documents\n",
    "csmap = {}\n",
    "for d in docs:\n",
    "    if d.chunksize not in csmap:\n",
    "        csmap[d.chunksize] = 0\n",
    "    csmap[d.chunksize] += 1\n",
    "\n",
    "print('Chunksizes {(chunksize: count)}: %s' % (csmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(All, Similar) pairs: (49995000, 92596) computed in 108.606 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Naively compare all n(n-1)/2 pairs to compute a full set of comparisons, together with the set of \n",
    "#         all (i,j) document list index pairs for matching/similar documents. \n",
    "\n",
    "import time\n",
    "\n",
    "all_doc_pair_count = 0\n",
    "naive_similar_doc_pairs = set()\n",
    "start_time = time.time()\n",
    "for i in range(0, len(docs)):\n",
    "    d0 = docs[i]\n",
    "    for j in range(i+1, len(docs)):\n",
    "        d1 = docs[j]\n",
    "        all_doc_pair_count += 1\n",
    "        compscore = d0.compare(d1)\n",
    "        if(compscore > 0 and compscore <= 100):\n",
    "            naive_similar_doc_pairs.add((i,j))\n",
    "            #print(\"%s, %s: %s\" % (docs[i], docs[j], compscore))\n",
    "\n",
    "print(\"(All, Similar) pairs: (%.0f, %s) computed in %.3f seconds\" % (all_doc_pair_count, len(naive_similar_doc_pairs), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(All, Similar) pairs: (49995000, 92596) computed in 92.600 seconds\n",
      "Naive/Faster pair sets match!\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Naively compare all n(n-1)/2 pairs to compute a full set of comparisons, but skip any pairs for which \n",
    "#         the relative chunk sizes are not equal, double, or half each other.\n",
    "\n",
    "all_doc_pair_count = 0\n",
    "faster_similar_doc_pairs = set()\n",
    "start_time = time.time()\n",
    "for i in range(0, len(docs)):\n",
    "    d0 = docs[i]\n",
    "    for j in range(i+1, len(docs)):\n",
    "        d1 = docs[j]\n",
    "        all_doc_pair_count += 1\n",
    "        if d0.comparable(d1):\n",
    "            compscore = d0.compare(d1)\n",
    "            if(compscore > 0 and compscore <= 100):\n",
    "                faster_similar_doc_pairs.add((i,j))\n",
    "                #print(\"%s, %s: %s\" % (docs[i], docs[j], compscore))\n",
    "\n",
    "print(\"(All, Similar) pairs: (%.0f, %s) computed in %.3f seconds\" % (all_doc_pair_count, len(faster_similar_doc_pairs), time.time() - start_time))\n",
    "\n",
    "if naive_similar_doc_pairs == faster_similar_doc_pairs:\n",
    "    print(\"Naive/Faster pair sets match!\")\n",
    "else:\n",
    "    print(\"Naive/Faster pair sets DON'T match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_matching_pairs() is an optimised method of computing matching/similar DocHash instances. Given a list of \n",
    "# DocHash instances as input, this method will compute all matching/similar pairs by building a lookup map based \n",
    "# on chunked ngrams (of length 7) which appear common to at least two DocHash instances in the input list.\n",
    "\n",
    "def get_matching_pairs(document_list, minimum_ssdeep_match_score=1):\n",
    "    \n",
    "    # create a map of chunk ngram -> {doc index} as a forward lookup to \n",
    "    # see, given a certain chunk ngram, which document indexes contain this \n",
    "    all_chunk_index_lookup_table = {}\n",
    "\n",
    "    # for every documet in the list\n",
    "    for d in document_list:\n",
    "        index = d.get_index()\n",
    "        # for every chunk ngram\n",
    "        for c in d.get_chunk_ngrams().union(d.get_double_chunk_ngrams()): \n",
    "            # add this document's index to the set against this chunk ngram\n",
    "            if c not in all_chunk_index_lookup_table:\n",
    "                all_chunk_index_lookup_table[c] = set()\n",
    "            all_chunk_index_lookup_table[c].add(index)\n",
    "\n",
    "    # iterate over the lookup map entries and retrieve all chunk ngram keys which are \n",
    "    # associated with more than one index, as these belong to DocHash instances which \n",
    "    # may be matching/similar. All other chunk ngram key entries associated with a \n",
    "    # single instance refer to singular DocHash instances with no comparable pair, \n",
    "    # which we can use as a filter when performing lookups for comparison canidates.\n",
    "    repeated_ngram_chunks = set()\n",
    "    for k in all_chunk_index_lookup_table:\n",
    "        if len(all_chunk_index_lookup_table[k]) > 1:\n",
    "            repeated_ngram_chunks.add(k)\n",
    "\n",
    "    # recompute the forward lookup map based only on the intersection\n",
    "    # of each document's chunk ngram set and the match_all_chunks set\n",
    "    all_chunk_index_lookup_table = {}\n",
    "    for d in document_list:\n",
    "        index = d.get_index()\n",
    "        for c in d.get_chunk_ngrams().union(d.get_double_chunk_ngrams()) & repeated_ngram_chunks: \n",
    "            if c not in all_chunk_index_lookup_table:\n",
    "                all_chunk_index_lookup_table[c] = set()\n",
    "            all_chunk_index_lookup_table[c].add(index)    \n",
    "\n",
    "    # finally, with the minimised lookup map, perform all comparisons for all docs\n",
    "    similar_doc_pairs = set()\n",
    "    for d0 in document_list:\n",
    "        # compute a set of indices of potential matches for d0 amongst the documents_list\n",
    "        # based on all index entries in the lookup map for every chunk ngram key \n",
    "        potential_match_indices = set()\n",
    "        for c in d0.get_chunk_ngrams().union(d0.get_double_chunk_ngrams()) & repeated_ngram_chunks:\n",
    "            potential_match_indices = potential_match_indices.union(all_chunk_index_lookup_table[c])\n",
    "        # for every potential match accumulated from the lookup map\n",
    "        for i in potential_match_indices:\n",
    "            # don't compare any index which is equal or lower than this document's one, as\n",
    "            # we only want to compute pairs in one direction, e.g., (1,2) but not (2,1) \n",
    "            if i <= d0.index:\n",
    "                continue\n",
    "            # retrieve the document from the list\n",
    "            d1 = document_list[i]\n",
    "            # if the documents are comparable based on the chunk size test\n",
    "            if d0.comparable(d1):\n",
    "                # then compute the expensive ssdeep hash comparison score\n",
    "                compscore = d0.compare(d1)\n",
    "                # if the resulting score lies within the necessary range\n",
    "                if(compscore >= minimum_ssdeep_match_score and compscore <= 100):\n",
    "                    # capture this index pair as being a matching/similar pair of documents\n",
    "                    similar_doc_pairs.add((d0.index, d1.index))\n",
    "    \n",
    "    # return any similar documents with a non-zero\n",
    "    return similar_doc_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(All, Similar) pairs: (49995000, 92596) computed in 3.667 seconds\n",
      "Naive/Faster/Optim similar pair sets match!\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Execute the get_matching_pairs() over the test list of documents to compare performance to the naive\n",
    "#         comparison methods.\n",
    "\n",
    "start_time = time.time()\n",
    "optim_similar_doc_pairs = get_matching_pairs(docs, minimum_ssdeep_match_score=1)\n",
    "print(\"(All, Similar) pairs: (%.0f, %s) computed in %.3f seconds\" % ((len(docs)*(len(docs)-1))/2, len(optim_similar_doc_pairs), time.time() - start_time))\n",
    "\n",
    "if naive_similar_doc_pairs == optim_similar_doc_pairs:\n",
    "    print(\"Naive/Faster/Optim similar pair sets match!\")\n",
    "else:\n",
    "    print(\"Naive/Faster/Optim similar pair sets DON'T match!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssdeep",
   "language": "python",
   "name": "ssdeep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
